{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4de4eb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-31T09:08:55.884814Z",
     "iopub.status.busy": "2026-01-31T09:08:55.884586Z",
     "iopub.status.idle": "2026-01-31T09:11:14.512391Z",
     "shell.execute_reply": "2026-01-31T09:11:14.511631Z"
    },
    "papermill": {
     "duration": 138.632874,
     "end_time": "2026-01-31T09:11:14.513882",
     "exception": false,
     "start_time": "2026-01-31T09:08:55.881008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== FOLD 1/4 ====================\n",
      "\n",
      "Epoch [1/30] | Train Loss: 2.9052 | Val Loss: 2.6610\n",
      "   >>> Epoch Best F1: 0.0000 at Thr: 0\n",
      "\n",
      "Epoch [2/30] | Train Loss: 2.3667 | Val Loss: 2.1572\n",
      "   >>> Epoch Best F1: 0.1392 at Thr: 0.1\n",
      "\n",
      "Epoch [3/30] | Train Loss: 1.9905 | Val Loss: 1.8795\n",
      "   >>> Epoch Best F1: 0.1170 at Thr: 0.1\n",
      "\n",
      "Epoch [4/30] | Train Loss: 1.6861 | Val Loss: 1.6009\n",
      "   >>> Epoch Best F1: 0.1460 at Thr: 0.2\n",
      "\n",
      "Epoch [5/30] | Train Loss: 1.4397 | Val Loss: 1.3513\n",
      "   >>> Epoch Best F1: 0.1525 at Thr: 0.2\n",
      "\n",
      "Epoch [6/30] | Train Loss: 1.2813 | Val Loss: 1.2193\n",
      "   >>> Epoch Best F1: 0.1434 at Thr: 0.2\n",
      "\n",
      "Epoch [7/30] | Train Loss: 1.1592 | Val Loss: 1.0975\n",
      "   >>> Epoch Best F1: 0.1714 at Thr: 0.3\n",
      "\n",
      "Epoch [8/30] | Train Loss: 1.0623 | Val Loss: 1.0622\n",
      "   >>> Epoch Best F1: 0.1592 at Thr: 0.2\n",
      "\n",
      "Epoch [9/30] | Train Loss: 1.0132 | Val Loss: 1.0093\n",
      "   >>> Epoch Best F1: 0.2338 at Thr: 0.3\n",
      "\n",
      "Epoch [10/30] | Train Loss: 0.9867 | Val Loss: 0.9823\n",
      "   >>> Epoch Best F1: 0.2778 at Thr: 0.3\n",
      "\n",
      "Epoch [11/30] | Train Loss: 0.9434 | Val Loss: 0.9420\n",
      "   >>> Epoch Best F1: 0.2963 at Thr: 0.3\n",
      "\n",
      "Epoch [12/30] | Train Loss: 0.8962 | Val Loss: 0.9199\n",
      "   >>> Epoch Best F1: 0.2778 at Thr: 0.3\n",
      "\n",
      "Epoch [13/30] | Train Loss: 0.8786 | Val Loss: 0.9082\n",
      "   >>> Epoch Best F1: 0.2192 at Thr: 0.3\n",
      "\n",
      "Epoch [14/30] | Train Loss: 0.8487 | Val Loss: 0.8949\n",
      "   >>> Epoch Best F1: 0.2362 at Thr: 0.3\n",
      "\n",
      "Epoch [15/30] | Train Loss: 0.8586 | Val Loss: 0.8921\n",
      "   >>> Epoch Best F1: 0.1965 at Thr: 0.3\n",
      "\n",
      "Epoch [16/30] | Train Loss: 0.8369 | Val Loss: 0.8740\n",
      "   >>> Epoch Best F1: 0.2167 at Thr: 0.3\n",
      "\n",
      "Epoch [17/30] | Train Loss: 0.8084 | Val Loss: 0.8469\n",
      "   >>> Epoch Best F1: 0.1910 at Thr: 0.3\n",
      "\n",
      "Epoch [18/30] | Train Loss: 0.8135 | Val Loss: 0.8502\n",
      "   >>> Epoch Best F1: 0.1622 at Thr: 0.3\n",
      "\n",
      "Epoch [19/30] | Train Loss: 0.7909 | Val Loss: 0.8238\n",
      "   >>> Epoch Best F1: 0.1935 at Thr: 0.4\n",
      "\n",
      "Epoch [20/30] | Train Loss: 0.7871 | Val Loss: 0.8369\n",
      "   >>> Epoch Best F1: 0.2118 at Thr: 0.4\n",
      "\n",
      "Epoch [21/30] | Train Loss: 0.7478 | Val Loss: 0.8367\n",
      "   >>> Epoch Best F1: 0.2338 at Thr: 0.4\n",
      "\n",
      "Epoch [22/30] | Train Loss: 0.7629 | Val Loss: 0.8136\n",
      "   >>> Epoch Best F1: 0.1935 at Thr: 0.4\n",
      "   >>> [SAVED] New Best Model for Fold 0 (F1: 0.1935)\n",
      "\n",
      "Epoch [23/30] | Train Loss: 0.7449 | Val Loss: 0.8229\n",
      "   >>> Epoch Best F1: 0.2000 at Thr: 0.4\n",
      "   >>> [SAVED] New Best Model for Fold 0 (F1: 0.2000)\n",
      "\n",
      "Epoch [24/30] | Train Loss: 0.7072 | Val Loss: 0.8187\n",
      "   >>> Epoch Best F1: 0.1803 at Thr: 0.4\n",
      "\n",
      "Epoch [25/30] | Train Loss: 0.7347 | Val Loss: 0.8071\n",
      "   >>> Epoch Best F1: 0.1923 at Thr: 0.5\n",
      "\n",
      "Epoch [26/30] | Train Loss: 0.7305 | Val Loss: 0.8442\n",
      "   >>> Epoch Best F1: 0.1418 at Thr: 0.2\n",
      "\n",
      "Epoch [27/30] | Train Loss: 0.7292 | Val Loss: 0.8089\n",
      "   >>> Epoch Best F1: 0.2016 at Thr: 0.4\n",
      "   >>> [SAVED] New Best Model for Fold 0 (F1: 0.2016)\n",
      "\n",
      "Epoch [28/30] | Train Loss: 0.6866 | Val Loss: 0.8070\n",
      "   >>> Epoch Best F1: 0.1765 at Thr: 0.4\n",
      "\n",
      "Epoch [29/30] | Train Loss: 0.7125 | Val Loss: 0.8078\n",
      "   >>> Epoch Best F1: 0.1957 at Thr: 0.4\n",
      "\n",
      "Epoch [30/30] | Train Loss: 0.6880 | Val Loss: 0.8148\n",
      "   >>> Epoch Best F1: 0.1895 at Thr: 0.4\n",
      "--- Finished Fold 1. Best F1: 0.2016 at Threshold: 0.4 ---\n",
      "\n",
      "==================== FOLD 2/4 ====================\n",
      "\n",
      "Epoch [1/30] | Train Loss: 3.0256 | Val Loss: 2.8235\n",
      "   >>> Epoch Best F1: 0.0000 at Thr: 0\n",
      "\n",
      "Epoch [2/30] | Train Loss: 2.4990 | Val Loss: 2.3412\n",
      "   >>> Epoch Best F1: 0.1310 at Thr: 0.1\n",
      "\n",
      "Epoch [3/30] | Train Loss: 2.0821 | Val Loss: 1.9081\n",
      "   >>> Epoch Best F1: 0.1220 at Thr: 0.1\n",
      "\n",
      "Epoch [4/30] | Train Loss: 1.7665 | Val Loss: 1.6646\n",
      "   >>> Epoch Best F1: 0.1222 at Thr: 0.1\n",
      "\n",
      "Epoch [5/30] | Train Loss: 1.5335 | Val Loss: 1.4873\n",
      "   >>> Epoch Best F1: 0.1333 at Thr: 0.2\n",
      "\n",
      "Epoch [6/30] | Train Loss: 1.3910 | Val Loss: 1.3049\n",
      "   >>> Epoch Best F1: 0.1336 at Thr: 0.1\n",
      "\n",
      "Epoch [7/30] | Train Loss: 1.2163 | Val Loss: 1.2077\n",
      "   >>> Epoch Best F1: 0.1392 at Thr: 0.2\n",
      "\n",
      "Epoch [8/30] | Train Loss: 1.1450 | Val Loss: 1.1047\n",
      "   >>> Epoch Best F1: 0.1635 at Thr: 0.2\n",
      "\n",
      "Epoch [9/30] | Train Loss: 1.0760 | Val Loss: 1.0637\n",
      "   >>> Epoch Best F1: 0.1609 at Thr: 0.2\n",
      "\n",
      "Epoch [10/30] | Train Loss: 1.0028 | Val Loss: 1.0068\n",
      "   >>> Epoch Best F1: 0.1775 at Thr: 0.3\n",
      "\n",
      "Epoch [11/30] | Train Loss: 0.9586 | Val Loss: 0.9497\n",
      "   >>> Epoch Best F1: 0.1972 at Thr: 0.3\n",
      "\n",
      "Epoch [12/30] | Train Loss: 0.9267 | Val Loss: 0.9154\n",
      "   >>> Epoch Best F1: 0.2041 at Thr: 0.3\n",
      "\n",
      "Epoch [13/30] | Train Loss: 0.9022 | Val Loss: 0.8872\n",
      "   >>> Epoch Best F1: 0.1972 at Thr: 0.4\n",
      "\n",
      "Epoch [14/30] | Train Loss: 0.8776 | Val Loss: 0.8767\n",
      "   >>> Epoch Best F1: 0.2024 at Thr: 0.3\n",
      "\n",
      "Epoch [15/30] | Train Loss: 0.8438 | Val Loss: 0.8547\n",
      "   >>> Epoch Best F1: 0.2466 at Thr: 0.4\n",
      "\n",
      "Epoch [16/30] | Train Loss: 0.8289 | Val Loss: 0.8513\n",
      "   >>> Epoch Best F1: 0.2267 at Thr: 0.3\n",
      "\n",
      "Epoch [17/30] | Train Loss: 0.8373 | Val Loss: 0.8366\n",
      "   >>> Epoch Best F1: 0.2683 at Thr: 0.4\n",
      "\n",
      "Epoch [18/30] | Train Loss: 0.8124 | Val Loss: 0.8271\n",
      "   >>> Epoch Best F1: 0.2456 at Thr: 0.4\n",
      "\n",
      "Epoch [19/30] | Train Loss: 0.7957 | Val Loss: 0.8153\n",
      "   >>> Epoch Best F1: 0.2921 at Thr: 0.4\n",
      "\n",
      "Epoch [20/30] | Train Loss: 0.7834 | Val Loss: 0.7917\n",
      "   >>> Epoch Best F1: 0.2656 at Thr: 0.4\n",
      "\n",
      "Epoch [21/30] | Train Loss: 0.7606 | Val Loss: 0.7943\n",
      "   >>> Epoch Best F1: 0.2759 at Thr: 0.4\n",
      "\n",
      "Epoch [22/30] | Train Loss: 0.7522 | Val Loss: 0.8015\n",
      "   >>> Epoch Best F1: 0.2778 at Thr: 0.4\n",
      "   >>> [SAVED] New Best Model for Fold 1 (F1: 0.2778)\n",
      "\n",
      "Epoch [23/30] | Train Loss: 0.7535 | Val Loss: 0.7875\n",
      "   >>> Epoch Best F1: 0.2523 at Thr: 0.4\n",
      "\n",
      "Epoch [24/30] | Train Loss: 0.7297 | Val Loss: 0.7883\n",
      "   >>> Epoch Best F1: 0.2553 at Thr: 0.4\n",
      "\n",
      "Epoch [25/30] | Train Loss: 0.7562 | Val Loss: 0.7809\n",
      "   >>> Epoch Best F1: 0.3056 at Thr: 0.5\n",
      "   >>> [SAVED] New Best Model for Fold 1 (F1: 0.3056)\n",
      "\n",
      "Epoch [26/30] | Train Loss: 0.7147 | Val Loss: 0.7765\n",
      "   >>> Epoch Best F1: 0.2449 at Thr: 0.5\n",
      "\n",
      "Epoch [27/30] | Train Loss: 0.7408 | Val Loss: 0.7713\n",
      "   >>> Epoch Best F1: 0.3226 at Thr: 0.5\n",
      "   >>> [SAVED] New Best Model for Fold 1 (F1: 0.3226)\n",
      "\n",
      "Epoch [28/30] | Train Loss: 0.6888 | Val Loss: 0.7694\n",
      "   >>> Epoch Best F1: 0.2597 at Thr: 0.5\n",
      "\n",
      "Epoch [29/30] | Train Loss: 0.6854 | Val Loss: 0.7585\n",
      "   >>> Epoch Best F1: 0.2716 at Thr: 0.5\n",
      "\n",
      "Epoch [30/30] | Train Loss: 0.6832 | Val Loss: 0.7512\n",
      "   >>> Epoch Best F1: 0.2826 at Thr: 0.5\n",
      "--- Finished Fold 2. Best F1: 0.3226 at Threshold: 0.5 ---\n",
      "\n",
      "==================== FOLD 3/4 ====================\n",
      "\n",
      "Epoch [1/30] | Train Loss: 2.6871 | Val Loss: 2.5361\n",
      "   >>> Epoch Best F1: 0.1333 at Thr: 0.1\n",
      "\n",
      "Epoch [2/30] | Train Loss: 2.2003 | Val Loss: 1.9829\n",
      "   >>> Epoch Best F1: 0.1318 at Thr: 0.1\n",
      "\n",
      "Epoch [3/30] | Train Loss: 1.8124 | Val Loss: 1.6081\n",
      "   >>> Epoch Best F1: 0.1432 at Thr: 0.1\n",
      "\n",
      "Epoch [4/30] | Train Loss: 1.5394 | Val Loss: 1.3976\n",
      "   >>> Epoch Best F1: 0.2105 at Thr: 0.2\n",
      "\n",
      "Epoch [5/30] | Train Loss: 1.3598 | Val Loss: 1.2403\n",
      "   >>> Epoch Best F1: 0.1477 at Thr: 0.2\n",
      "\n",
      "Epoch [6/30] | Train Loss: 1.2380 | Val Loss: 1.1730\n",
      "   >>> Epoch Best F1: 0.1503 at Thr: 0.2\n",
      "\n",
      "Epoch [7/30] | Train Loss: 1.1705 | Val Loss: 1.0666\n",
      "   >>> Epoch Best F1: 0.1833 at Thr: 0.2\n",
      "\n",
      "Epoch [8/30] | Train Loss: 1.1062 | Val Loss: 1.0177\n",
      "   >>> Epoch Best F1: 0.1868 at Thr: 0.2\n",
      "\n",
      "Epoch [9/30] | Train Loss: 1.0269 | Val Loss: 0.9909\n",
      "   >>> Epoch Best F1: 0.2025 at Thr: 0.2\n",
      "\n",
      "Epoch [10/30] | Train Loss: 1.0252 | Val Loss: 0.9619\n",
      "   >>> Epoch Best F1: 0.1933 at Thr: 0.2\n",
      "\n",
      "Epoch [11/30] | Train Loss: 0.9942 | Val Loss: 0.9622\n",
      "   >>> Epoch Best F1: 0.2083 at Thr: 0.3\n",
      "\n",
      "Epoch [12/30] | Train Loss: 0.9930 | Val Loss: 0.9429\n",
      "   >>> Epoch Best F1: 0.2400 at Thr: 0.3\n",
      "\n",
      "Epoch [13/30] | Train Loss: 0.9564 | Val Loss: 0.9192\n",
      "   >>> Epoch Best F1: 0.2232 at Thr: 0.2\n",
      "\n",
      "Epoch [14/30] | Train Loss: 0.9466 | Val Loss: 0.9014\n",
      "   >>> Epoch Best F1: 0.2143 at Thr: 0.3\n",
      "\n",
      "Epoch [15/30] | Train Loss: 0.9317 | Val Loss: 0.8811\n",
      "   >>> Epoch Best F1: 0.2177 at Thr: 0.3\n",
      "\n",
      "Epoch [16/30] | Train Loss: 0.9199 | Val Loss: 0.9060\n",
      "   >>> Epoch Best F1: 0.2326 at Thr: 0.3\n",
      "\n",
      "Epoch [17/30] | Train Loss: 0.8979 | Val Loss: 0.8744\n",
      "   >>> Epoch Best F1: 0.2286 at Thr: 0.3\n",
      "\n",
      "Epoch [18/30] | Train Loss: 0.8920 | Val Loss: 0.8730\n",
      "   >>> Epoch Best F1: 0.2044 at Thr: 0.3\n",
      "\n",
      "Epoch [19/30] | Train Loss: 0.8964 | Val Loss: 0.8703\n",
      "   >>> Epoch Best F1: 0.2245 at Thr: 0.3\n",
      "\n",
      "Epoch [20/30] | Train Loss: 0.9019 | Val Loss: 0.9029\n",
      "   >>> Epoch Best F1: 0.2571 at Thr: 0.3\n",
      "\n",
      "Epoch [21/30] | Train Loss: 0.8645 | Val Loss: 0.8625\n",
      "   >>> Epoch Best F1: 0.2737 at Thr: 0.3\n",
      "\n",
      "Epoch [22/30] | Train Loss: 0.8847 | Val Loss: 0.8394\n",
      "   >>> Epoch Best F1: 0.2807 at Thr: 0.3\n",
      "   >>> [SAVED] New Best Model for Fold 2 (F1: 0.2807)\n",
      "\n",
      "Epoch [23/30] | Train Loss: 0.8783 | Val Loss: 0.8456\n",
      "   >>> Epoch Best F1: 0.2435 at Thr: 0.3\n",
      "\n",
      "Epoch [24/30] | Train Loss: 0.8582 | Val Loss: 0.8601\n",
      "   >>> Epoch Best F1: 0.2353 at Thr: 0.3\n",
      "\n",
      "Epoch [25/30] | Train Loss: 0.8345 | Val Loss: 0.8670\n",
      "   >>> Epoch Best F1: 0.2268 at Thr: 0.3\n",
      "\n",
      "Epoch [26/30] | Train Loss: 0.8494 | Val Loss: 0.8347\n",
      "   >>> Epoch Best F1: 0.2500 at Thr: 0.3\n",
      "\n",
      "Epoch [27/30] | Train Loss: 0.8327 | Val Loss: 0.8225\n",
      "   >>> Epoch Best F1: 0.2500 at Thr: 0.4\n",
      "\n",
      "Epoch [28/30] | Train Loss: 0.8273 | Val Loss: 0.8282\n",
      "   >>> Epoch Best F1: 0.2576 at Thr: 0.3\n",
      "\n",
      "Epoch [29/30] | Train Loss: 0.8270 | Val Loss: 0.8080\n",
      "   >>> Epoch Best F1: 0.2857 at Thr: 0.4\n",
      "   >>> [SAVED] New Best Model for Fold 2 (F1: 0.2857)\n",
      "\n",
      "Epoch [30/30] | Train Loss: 0.8187 | Val Loss: 0.8297\n",
      "   >>> Epoch Best F1: 0.2479 at Thr: 0.3\n",
      "--- Finished Fold 3. Best F1: 0.2857 at Threshold: 0.4 ---\n",
      "\n",
      "==================== FOLD 4/4 ====================\n",
      "\n",
      "Epoch [1/30] | Train Loss: 3.1427 | Val Loss: 2.8406\n",
      "   >>> Epoch Best F1: 0.0303 at Thr: 0.1\n",
      "\n",
      "Epoch [2/30] | Train Loss: 2.5895 | Val Loss: 2.3334\n",
      "   >>> Epoch Best F1: 0.1455 at Thr: 0.1\n",
      "\n",
      "Epoch [3/30] | Train Loss: 2.1413 | Val Loss: 1.9502\n",
      "   >>> Epoch Best F1: 0.1147 at Thr: 0.1\n",
      "\n",
      "Epoch [4/30] | Train Loss: 1.8171 | Val Loss: 1.6066\n",
      "   >>> Epoch Best F1: 0.1201 at Thr: 0.1\n",
      "\n",
      "Epoch [5/30] | Train Loss: 1.5477 | Val Loss: 1.3908\n",
      "   >>> Epoch Best F1: 0.1983 at Thr: 0.2\n",
      "\n",
      "Epoch [6/30] | Train Loss: 1.3487 | Val Loss: 1.2435\n",
      "   >>> Epoch Best F1: 0.2086 at Thr: 0.2\n",
      "\n",
      "Epoch [7/30] | Train Loss: 1.2185 | Val Loss: 1.1290\n",
      "   >>> Epoch Best F1: 0.1939 at Thr: 0.2\n",
      "\n",
      "Epoch [8/30] | Train Loss: 1.1183 | Val Loss: 1.0624\n",
      "   >>> Epoch Best F1: 0.2000 at Thr: 0.2\n",
      "\n",
      "Epoch [9/30] | Train Loss: 1.0320 | Val Loss: 1.0180\n",
      "   >>> Epoch Best F1: 0.1944 at Thr: 0.2\n",
      "\n",
      "Epoch [10/30] | Train Loss: 0.9853 | Val Loss: 0.9828\n",
      "   >>> Epoch Best F1: 0.1833 at Thr: 0.2\n",
      "\n",
      "Epoch [11/30] | Train Loss: 0.9314 | Val Loss: 0.9396\n",
      "   >>> Epoch Best F1: 0.2162 at Thr: 0.3\n",
      "\n",
      "Epoch [12/30] | Train Loss: 0.9181 | Val Loss: 0.9148\n",
      "   >>> Epoch Best F1: 0.2059 at Thr: 0.3\n",
      "\n",
      "Epoch [13/30] | Train Loss: 0.8692 | Val Loss: 0.9187\n",
      "   >>> Epoch Best F1: 0.1837 at Thr: 0.3\n",
      "\n",
      "Epoch [14/30] | Train Loss: 0.8634 | Val Loss: 0.9205\n",
      "   >>> Epoch Best F1: 0.1800 at Thr: 0.2\n",
      "\n",
      "Epoch [15/30] | Train Loss: 0.8334 | Val Loss: 0.8884\n",
      "   >>> Epoch Best F1: 0.2065 at Thr: 0.3\n",
      "\n",
      "Epoch [16/30] | Train Loss: 0.8206 | Val Loss: 0.8628\n",
      "   >>> Epoch Best F1: 0.2353 at Thr: 0.4\n",
      "\n",
      "Epoch [17/30] | Train Loss: 0.7928 | Val Loss: 0.8936\n",
      "   >>> Epoch Best F1: 0.1575 at Thr: 0.3\n",
      "\n",
      "Epoch [18/30] | Train Loss: 0.8250 | Val Loss: 0.8854\n",
      "   >>> Epoch Best F1: 0.1704 at Thr: 0.2\n",
      "\n",
      "Epoch [19/30] | Train Loss: 0.7973 | Val Loss: 0.8873\n",
      "   >>> Epoch Best F1: 0.1659 at Thr: 0.2\n",
      "\n",
      "Epoch [20/30] | Train Loss: 0.7863 | Val Loss: 0.9156\n",
      "   >>> Epoch Best F1: 0.2000 at Thr: 0.2\n",
      "\n",
      "Epoch [21/30] | Train Loss: 0.7905 | Val Loss: 0.8825\n",
      "   >>> Epoch Best F1: 0.1771 at Thr: 0.2\n",
      "\n",
      "Epoch [22/30] | Train Loss: 0.7780 | Val Loss: 0.8763\n",
      "   >>> Epoch Best F1: 0.1481 at Thr: 0.2\n",
      "   >>> [SAVED] New Best Model for Fold 3 (F1: 0.1481)\n",
      "\n",
      "Epoch [23/30] | Train Loss: 0.7679 | Val Loss: 0.8687\n",
      "   >>> Epoch Best F1: 0.1732 at Thr: 0.3\n",
      "   >>> [SAVED] New Best Model for Fold 3 (F1: 0.1732)\n",
      "\n",
      "Epoch [24/30] | Train Loss: 0.7660 | Val Loss: 0.8700\n",
      "   >>> Epoch Best F1: 0.1524 at Thr: 0.3\n",
      "\n",
      "Epoch [25/30] | Train Loss: 0.7827 | Val Loss: 0.8665\n",
      "   >>> Epoch Best F1: 0.1597 at Thr: 0.2\n",
      "\n",
      "Epoch [26/30] | Train Loss: 0.7563 | Val Loss: 0.8736\n",
      "   >>> Epoch Best F1: 0.1466 at Thr: 0.2\n",
      "\n",
      "Epoch [27/30] | Train Loss: 0.7599 | Val Loss: 0.8762\n",
      "   >>> Epoch Best F1: 0.1441 at Thr: 0.2\n",
      "\n",
      "Epoch [28/30] | Train Loss: 0.7548 | Val Loss: 0.8608\n",
      "   >>> Epoch Best F1: 0.1511 at Thr: 0.2\n",
      "\n",
      "Epoch [29/30] | Train Loss: 0.7239 | Val Loss: 0.8727\n",
      "   >>> Epoch Best F1: 0.1565 at Thr: 0.2\n",
      "\n",
      "Epoch [30/30] | Train Loss: 0.7516 | Val Loss: 0.8761\n",
      "   >>> Epoch Best F1: 0.1475 at Thr: 0.3\n",
      "--- Finished Fold 4. Best F1: 0.1732 at Threshold: 0.3 ---\n",
      "Best Thresholds per fold: [0.4, 0.5, 0.4, 0.3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.stats import skew, kurtosis \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import linregress\n",
    "import gc\n",
    "\n",
    "BASE_PATH = '/kaggle/input/mallorn-dataset'\n",
    "SEQ_LEN = 150        \n",
    "BATCH_SIZE = 64      \n",
    "NUM_EPOCHS = 30   \n",
    "LEARNING_RATE = 1e-5\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def extract_features(fluxes, times, flux_errs, filters):\n",
    "    if len(fluxes) == 0: \n",
    "        return [0]*10\n",
    "    f = fluxes\n",
    "    t = times\n",
    "    fil = np.array(filters)\n",
    "    max_f = np.max(f)\n",
    "    min_f = np.min(f)\n",
    "    mean = np.mean(f) \n",
    "    std = np.std(f)\n",
    "    sk = skew(f) if std > 0 else 0 \n",
    "    kt = kurtosis(f) if std > 0 else 0\n",
    "    amplitude = (np.max(f) - np.min(f)) / 2\n",
    "\n",
    "    flux_by_band = {b: [] for b in range(6)}\n",
    "    for val, band in zip(f, filters):\n",
    "        flux_by_band[band].append(val)\n",
    "        \n",
    "    means_band = {b: np.mean(vals) if len(vals)>0 else np.nan for b, vals in flux_by_band.items()}\n",
    "    \n",
    "    u_g = np.nan_to_num(means_band[0] - means_band[1]) \n",
    "    g_r = np.nan_to_num(means_band[1] - means_band[2]) \n",
    "    r_i = np.nan_to_num(means_band[2] - means_band[3]) \n",
    "    \n",
    "    flux_ratio = max_f / (np.abs(np.median(f)) + 1e-6)\n",
    "    \n",
    "    idx_max = np.argmax(f)\n",
    "    time_max = t[idx_max]\n",
    "    slope = 0\n",
    "    if idx_max < len(f) - 1:\n",
    "        decay_flux = f[idx_max:]\n",
    "        decay_time = t[idx_max:]\n",
    "        if len(np.unique(decay_time)) > 1:\n",
    "            try:\n",
    "                slope, _, _, _, _ = linregress(decay_time, decay_flux)\n",
    "                if np.isnan(slope): slope = 0\n",
    "            except ValueError:\n",
    "                slope = 0 \n",
    "        else:\n",
    "            slope = 0\n",
    "\n",
    "    n = len(f)\n",
    "    if n > 1 and std > 0:\n",
    "        residuals = (f - mean) / (std + 1e-6)\n",
    "        stetson_k = (1 / np.sqrt(n)) * np.sum(np.abs(residuals)) / np.sqrt(np.mean(residuals**2))\n",
    "    else:\n",
    "        stetson_k = 0\n",
    "    t_peak = t[idx_max]\n",
    "    t_rise = t_peak - t[0]\n",
    "    t_decay = t[-1] - t_peak\n",
    "    rise_decay_ratio = 0\n",
    "    if t_decay > 0:\n",
    "        rise_decay_ratio = t_rise / (t_decay + 1e-6)\n",
    "\n",
    "    power_law_index = 0\n",
    "    \n",
    "    if idx_max < n - 2: \n",
    "        post_peak_f = f[idx_max+1:]\n",
    "        post_peak_t = t[idx_max+1:]\n",
    "        valid_mask = post_peak_f > 0\n",
    "        if np.sum(valid_mask) > 1: \n",
    "            y_log = np.log(post_peak_f[valid_mask])\n",
    "            x_log = np.log(post_peak_t[valid_mask] - t_peak + 1.0) \n",
    "            if len(np.unique(x_log)) > 1:\n",
    "                try:\n",
    "                    p_index, _, _, _, _ = linregress(x_log, y_log)\n",
    "                    if not np.isnan(p_index):\n",
    "                        power_law_index = p_index\n",
    "                except: power_law_index = 0\n",
    "    \n",
    "    mask_peak = (t <= t_peak + 10)\n",
    "    mask_tail = (t > t_peak + 20)\n",
    "    def get_color_gr(mask):\n",
    "        f_masked = f[mask]\n",
    "        fil_masked = fil[mask]\n",
    "        g_vals = f_masked[fil_masked == 1] \n",
    "        r_vals = f_masked[fil_masked == 2] \n",
    "        if len(g_vals) > 0 and len(r_vals) > 0:\n",
    "            return np.mean(g_vals) - np.mean(r_vals)\n",
    "        return 0.0 \n",
    "    gr_peak = get_color_gr(mask_peak)\n",
    "    gr_tail = get_color_gr(mask_tail)\n",
    "    \n",
    "    delta_color_gr = gr_tail - gr_peak if gr_tail != 0 and gr_peak != 0 else 0\n",
    "    half_max_flux = max_f / 2.0\n",
    "    width_half_max = 0\n",
    "\n",
    "    mask_high = f > half_max_flux\n",
    "    if np.sum(mask_high) >= 2:\n",
    "        t_high = t[mask_high]\n",
    "        width_half_max = t_high.max() - t_high.min()\n",
    "    else:\n",
    "        width_half_max = 0\n",
    "        \n",
    "    t_rise_50 = t_peak - t[f > half_max_flux].min() if np.sum(f > half_max_flux) > 0 else 0\n",
    "    t_fall_50 = t[f > half_max_flux].max() - t_peak if np.sum(f > half_max_flux) > 0 else 0\n",
    "    \n",
    "    asymmetry_50 = t_fall_50 / (t_rise_50 + 1e-6)\n",
    "    \n",
    "    return [mean, std, sk, kt, amplitude, \n",
    "            u_g, g_r, r_i, flux_ratio, len(f),\n",
    "            means_band[0], means_band[1], means_band[2], slope, stetson_k,\n",
    "            rise_decay_ratio, power_law_index, delta_color_gr, width_half_max, asymmetry_50]\n",
    "\n",
    "\n",
    "df_log = pd.read_csv('/kaggle/input/mallorn-dataset/train_log.csv')\n",
    "TRAIN_Z_MAX = df_log['Z'].max()\n",
    "TRAIN_EBV_MAX = df_log['EBV'].max()\n",
    "df_log['Z'] = df_log['Z'] / (df_log['Z'].max() + 1e-6)\n",
    "df_log['EBV'] = df_log['EBV'] / (df_log['EBV'].max() + 1e-6)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_log['label_encoded'] = label_encoder.fit_transform(df_log['SpecType'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "tde_index = label_encoder.transform(['TDE'])[0]\n",
    "\n",
    "class_weights = torch.ones(num_classes, dtype=torch.float32).to(device)\n",
    "\n",
    "class_weights[tde_index] = 5\n",
    "\n",
    "\n",
    "static_dict = df_log.set_index('object_id')[['Z', 'EBV']].T.to_dict('list')\n",
    "id_to_label = dict(zip(df_log['object_id'], df_log['label_encoded']))\n",
    "\n",
    "all_X_seq = []   \n",
    "all_X_static = [] \n",
    "all_y = []\n",
    "band_map = {'u': 0, 'g': 1, 'r': 2, 'i': 3, 'z': 4, 'y': 5}\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "            \n",
    "for i in range(1, 21): \n",
    "    folder_name = f\"split_{i:02d}\"\n",
    "    file_path = f\"{BASE_PATH}/{folder_name}/train_full_lightcurves.csv\"\n",
    "    df_chunk = pd.read_csv(file_path, usecols=['object_id', 'Flux', 'Time (MJD)', 'Flux_err', 'Filter'], dtype={'Flux': 'float32', 'Time (MJD)': 'float32'})\n",
    "    grouped = df_chunk.groupby('object_id').agg({'Flux': list, 'Time (MJD)': list, 'Flux_err': list, 'Filter': list}).to_dict(orient='index')\n",
    "    \n",
    "    for obj_id, data in grouped.items():\n",
    "        fluxes = np.array(data['Flux'], dtype=np.float32)\n",
    "        times = np.array(data['Time (MJD)'], dtype=np.float32)\n",
    "        filters = np.array([band_map.get(f, 0) for f in data['Filter']])\n",
    "        flux_errs = np.array(data['Flux_err'], dtype=np.float32)\n",
    "        \n",
    "        sorted_idx = np.argsort(times)\n",
    "        times = times[sorted_idx]\n",
    "        fluxes = fluxes[sorted_idx]\n",
    "        flux_errs = flux_errs[sorted_idx]\n",
    "        filters = filters[sorted_idx]\n",
    "        \n",
    "        phys_feats = extract_features(fluxes, times, flux_errs, filters)\n",
    "        \n",
    "        f_mean = np.mean(fluxes)\n",
    "        f_std = np.std(fluxes) + 1e-6\n",
    "        fluxes = (fluxes - f_mean) / f_std\n",
    "        fluxes = fluxes.tolist()\n",
    "        \n",
    "        flux_errs = flux_errs / f_std\n",
    "        flux_errs = flux_errs.tolist()\n",
    "        \n",
    "        start_time = times[0]\n",
    "        norm_times = times - start_time\n",
    "        norm_times = norm_times.tolist()\n",
    "\n",
    "        filters = filters.tolist()\n",
    "\n",
    "        if len(fluxes) > SEQ_LEN:\n",
    "            fluxes = fluxes[:SEQ_LEN]\n",
    "            norm_times = norm_times[:SEQ_LEN]\n",
    "            flux_errs = flux_errs[:SEQ_LEN]\n",
    "            filters = filters[:SEQ_LEN]\n",
    "        else:\n",
    "            pad_len = SEQ_LEN - len(fluxes)\n",
    "            fluxes = fluxes + [0.0] * pad_len\n",
    "            norm_times = norm_times + [0.0] * pad_len\n",
    "            flux_errs = flux_errs + [0.0] * pad_len \n",
    "            filters = filters + [0] * pad_len\n",
    "            \n",
    "        combined_seq = [[f, t, err, filt] for f, t, err, filt in zip(fluxes, norm_times, flux_errs, filters)]\n",
    "        current_static = static_dict[obj_id] + phys_feats\n",
    "        \n",
    "        all_X_seq.append(combined_seq)\n",
    "        all_X_static.append(current_static)\n",
    "        all_y.append(id_to_label[obj_id])\n",
    "        \n",
    "    del df_chunk, grouped\n",
    "    gc.collect()\n",
    "\n",
    "X_seq_tensor = torch.tensor(all_X_seq, dtype=torch.float32)\n",
    "X_seq_tensor = torch.nan_to_num(X_seq_tensor, nan=0.0)\n",
    "\n",
    "X_static_tensor = torch.tensor(all_X_static, dtype=torch.float32)\n",
    "X_static_tensor = torch.nan_to_num(X_static_tensor, nan=0.0)\n",
    "\n",
    "mean_static = X_static_tensor.mean(dim=0)\n",
    "std_static = X_static_tensor.std(dim=0) + 1e-6\n",
    "X_static_tensor = (X_static_tensor - mean_static) / std_static\n",
    "\n",
    "y_tensor = torch.tensor(all_y, dtype=torch.long)\n",
    "\n",
    "STATIC_INPUT_DIM = X_static_tensor.shape[1] \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500): \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, seq_len, static_input_dim, d_model, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.filter_embedding = nn.Embedding(num_embeddings=6, embedding_dim=16)\n",
    "        \n",
    "        total_input_dim = 3 + 16 + static_input_dim \n",
    "        self.input_proj = nn.Linear(total_input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len + 50)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = d_model,\n",
    "            nhead = 4,\n",
    "            dim_feedforward=256,\n",
    "            batch_first=True,\n",
    "            dropout = 0.3\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers = 4)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, x_static):\n",
    "        batch_size, seq_len, _ = x_seq.shape\n",
    "        \n",
    "        numeric_feats = x_seq[:, :, :3]\n",
    "        filter_ids = x_seq[:, :, 3].long()\n",
    "        filter_emb = self.filter_embedding(filter_ids)\n",
    "        \n",
    "        padding_mask = (numeric_feats[:, :, 0] != 0).float().unsqueeze(-1)\n",
    "        \n",
    "        static_expanded = x_static.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        static_expanded = static_expanded * padding_mask\n",
    "        \n",
    "        combined_input = torch.cat([numeric_feats, filter_emb, static_expanded], dim=-1)\n",
    "        \n",
    "        x = self.input_proj(combined_input)\n",
    "        \n",
    "        src_key_padding_mask = (numeric_feats[:, :, 0] == 0)\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        x = torch.nan_to_num(x, nan=0.0)\n",
    "        x_max = x.max(dim=1)[0]\n",
    "        x_mean = x.mean(dim=1)\n",
    "        x_feat = x_max + x_mean\n",
    "        \n",
    "        res = self.fc(x_feat)\n",
    "        return res\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, static_input_dim, hidden_dim, num_classes, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.filter_embedding = nn.Embedding(num_embeddings=6, embedding_dim=16)\n",
    "        \n",
    "        input_dim = 3 + 16 \n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        self.static_proj = nn.Sequential(\n",
    "            nn.Linear(static_input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        combined_dim = (hidden_dim * 4) + hidden_dim\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, x_static):\n",
    "        numeric_feats = x_seq[:, :, :3]\n",
    "        filter_ids = x_seq[:, :, 3].long()\n",
    "        filter_emb = self.filter_embedding(filter_ids)\n",
    "        lstm_input = torch.cat([numeric_feats, filter_emb], dim=-1)\n",
    "        out, _ = self.lstm(lstm_input)\n",
    "        \n",
    "        avg_pool = torch.mean(out, dim=1)\n",
    "        max_pool, _ = torch.max(out, dim=1)\n",
    "        seq_feat = torch.cat([avg_pool, max_pool], dim=1) \n",
    "        \n",
    "        static_feat = self.static_proj(x_static) \n",
    "        \n",
    "        final_feat = torch.cat([seq_feat, static_feat], dim=1)\n",
    "        return self.fc(final_feat)\n",
    "        \n",
    "N_FOLDS = 4\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "fold_best_thresholds = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_seq_tensor, y_tensor.cpu().numpy())):\n",
    "    print(f\"\\n{'='*20} FOLD {fold+1}/{N_FOLDS} {'='*20}\")\n",
    "\n",
    "    X_seq_train_fold = X_seq_tensor[train_idx]\n",
    "    X_static_train_fold = X_static_tensor[train_idx]\n",
    "    y_train_fold = y_tensor[train_idx]\n",
    "    \n",
    "    X_seq_val_fold = X_seq_tensor[val_idx]\n",
    "    X_static_val_fold = X_static_tensor[val_idx]\n",
    "    y_val_fold = y_tensor[val_idx]\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(X_seq_train_fold, X_static_train_fold, y_train_fold)\n",
    "    val_ds = torch.utils.data.TensorDataset(X_seq_val_fold, X_static_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = LSTMModel(\n",
    "        static_input_dim=STATIC_INPUT_DIM,\n",
    "        hidden_dim=128,      \n",
    "        num_classes=num_classes,\n",
    "        num_layers=2         \n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=2.0).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4) \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    fold_best_f1 = 0.0\n",
    "    fold_best_thr = 0.5\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_seq, batch_static, batch_y in train_loader:\n",
    "            batch_seq, batch_static, batch_y = batch_seq.to(device), batch_static.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_seq, batch_static)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            if torch.isnan(loss): continue\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_probs = []   \n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_seq, batch_static, batch_y in val_loader:\n",
    "                batch_seq, batch_static, batch_y = batch_seq.to(device), batch_static.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_seq, batch_static)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)[:, tde_index]\n",
    "                \n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_targets.extend((batch_y == tde_index).cpu().numpy().astype(int))\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")       \n",
    "        thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        \n",
    "        all_probs = np.array(all_probs)\n",
    "        all_targets = np.array(all_targets)\n",
    "        \n",
    "        epoch_best_f1 = 0\n",
    "        epoch_best_thr = 0\n",
    "        for thr in thresholds:\n",
    "            preds = (all_probs > thr).astype(int)\n",
    "            f1 = f1_score(all_targets, preds, zero_division=0)\n",
    "            if f1 > epoch_best_f1:\n",
    "                epoch_best_f1 = f1\n",
    "                epoch_best_thr = thr\n",
    "        print(f\"   >>> Epoch Best F1: {epoch_best_f1:.4f} at Thr: {epoch_best_thr}\")\n",
    "        if epoch_best_f1 > fold_best_f1 and epoch > 20:\n",
    "            fold_best_f1 = epoch_best_f1\n",
    "            fold_best_thr = epoch_best_thr\n",
    "            \n",
    "            save_name = f'best_model_fold_{fold}.pth'\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'threshold': fold_best_thr,\n",
    "                'f1_score': fold_best_f1\n",
    "            }, save_name)\n",
    "            \n",
    "            print(f\"   >>> [SAVED] New Best Model for Fold {fold} (F1: {fold_best_f1:.4f})\")\n",
    "            \n",
    "    print(f\"--- Finished Fold {fold+1}. Best F1: {fold_best_f1:.4f} at Threshold: {fold_best_thr} ---\")\n",
    "    fold_best_thresholds.append(fold_best_thr)\n",
    "    del model, optimizer, train_loader, val_loader, train_ds, val_ds\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Best Thresholds per fold:\", fold_best_thresholds)\n",
    "np.save('folds_thresholds.npy', np.array(fold_best_thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5895eca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T09:11:14.526862Z",
     "iopub.status.busy": "2026-01-31T09:11:14.526487Z",
     "iopub.status.idle": "2026-01-31T09:11:40.684681Z",
     "shell.execute_reply": "2026-01-31T09:11:40.683884Z"
    },
    "papermill": {
     "duration": 26.166329,
     "end_time": "2026-01-31T09:11:40.686219",
     "exception": false,
     "start_time": "2026-01-31T09:11:14.519890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tìm thấy 853 TDE trên tổng số 7135 mẫu.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import skew, kurtosis\n",
    "import gc\n",
    "\n",
    "\n",
    "BASE_PATH = '/kaggle/input/mallorn-dataset'\n",
    "SEQ_LEN = 150        \n",
    "BATCH_SIZE_TEST = 128      \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FINAL_THRESHOLD = 0.35\n",
    "\n",
    "\n",
    "df_train_full = pd.read_csv(f'{BASE_PATH}/train_log.csv')\n",
    "unique_labels = sorted(df_train_full['SpecType'].unique())\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "\n",
    "TRAIN_Z_MAX = df_train_full['Z'].max()\n",
    "TRAIN_EBV_MAX = df_train_full['EBV'].max()\n",
    "static_dict_train = df_train_full.set_index('object_id')[['Z', 'EBV']].T.to_dict('list')\n",
    "\n",
    "df_test_log = pd.read_csv(f'{BASE_PATH}/test_log.csv')\n",
    "df_test_log['Z'] = df_test_log['Z'] / (TRAIN_Z_MAX + 1e-6)\n",
    "df_test_log['EBV'] = df_test_log['EBV'] / (TRAIN_EBV_MAX + 1e-6)\n",
    "static_dict_test = df_test_log.set_index('object_id')[['Z', 'EBV']].T.to_dict('list')\n",
    "\n",
    "models = []\n",
    "for fold in range(4):\n",
    "    model = LSTMModel(\n",
    "        static_input_dim=STATIC_INPUT_DIM,\n",
    "        hidden_dim=128,      \n",
    "        num_classes=num_classes,\n",
    "        num_layers=2         \n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(f'best_model_fold_{fold}.pth', map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "results = {}\n",
    "mean_static = mean_static.to(DEVICE)\n",
    "std_static = std_static.to(DEVICE)\n",
    "with torch.no_grad(): \n",
    "    for i in range(1, 21): \n",
    "        folder_name = f\"split_{i:02d}\"\n",
    "        file_path = f\"{BASE_PATH}/{folder_name}/test_full_lightcurves.csv\"\n",
    "        df_chunk = pd.read_csv(file_path, usecols=['object_id', 'Flux', 'Time (MJD)', 'Flux_err', 'Filter'], \n",
    "                               dtype={'Flux': 'float32', 'Time (MJD)': 'float32'})\n",
    "        \n",
    "        grouped = df_chunk.groupby('object_id').agg({\n",
    "            'Flux': list, 'Time (MJD)': list, 'Flux_err': list, 'Filter': list\n",
    "        }).to_dict(orient='index')\n",
    "        \n",
    "        batch_ids, batch_seq, batch_static = [], [], []\n",
    "        \n",
    "        for obj_id, data in grouped.items():\n",
    "            fluxes = np.array(data['Flux'], dtype=np.float32)\n",
    "            times = np.array(data['Time (MJD)'], dtype=np.float32)\n",
    "            flux_errs = np.array(data['Flux_err'], dtype=np.float32)\n",
    "            filters = np.array([band_map.get(f, 0) for f in data['Filter']])\n",
    "\n",
    "            sorted_idx = np.argsort(times)\n",
    "            times = times[sorted_idx]\n",
    "            fluxes = fluxes[sorted_idx]\n",
    "            flux_errs = flux_errs[sorted_idx]\n",
    "            filters = filters[sorted_idx]\n",
    "            \n",
    "            phys_feats = extract_features(fluxes, times, flux_errs, filters)\n",
    "            \n",
    "            f_std = np.std(fluxes) + 1e-6\n",
    "            fluxes = ((fluxes - np.mean(fluxes)) / f_std).tolist()\n",
    "            flux_errs = (flux_errs / f_std).tolist()\n",
    "            norm_times = (times - times[0]).tolist()\n",
    "            filters = filters.tolist()\n",
    "            \n",
    "            if len(fluxes) > SEQ_LEN:\n",
    "                fluxes, norm_times, flux_errs, filters = fluxes[:SEQ_LEN], norm_times[:SEQ_LEN], flux_errs[:SEQ_LEN], filters[:SEQ_LEN]\n",
    "            else:\n",
    "                pad = SEQ_LEN - len(fluxes)\n",
    "                fluxes += [0]*pad; norm_times += [0]*pad; flux_errs += [0]*pad; filters += [0]*pad\n",
    "            \n",
    "            batch_ids.append(obj_id)\n",
    "            batch_seq.append([[f, t, err, filt] for f, t, err, filt in zip(fluxes, norm_times, flux_errs, filters)])\n",
    "            batch_static.append(static_dict_test[obj_id] + phys_feats)\n",
    "            \n",
    "            if len(batch_ids) >= BATCH_SIZE_TEST:\n",
    "                X_seq = torch.tensor(batch_seq, dtype=torch.float32).to(DEVICE)\n",
    "                X_stat = torch.tensor(batch_static, dtype=torch.float32).to(DEVICE)\n",
    "                \n",
    "                X_seq = torch.nan_to_num(X_seq, nan=0.0)\n",
    "                X_stat = torch.nan_to_num(X_stat, nan=0.0)\n",
    "                X_stat = (X_stat - mean_static) / std_static\n",
    "                X_stat = torch.nan_to_num(X_stat, nan=0.0) \n",
    "                \n",
    "                avg_probs = np.zeros(len(batch_ids))\n",
    "                for model in models:\n",
    "                    avg_probs += torch.softmax(model(X_seq, X_stat), dim=1)[:, tde_index].cpu().numpy()\n",
    "                avg_probs /= len(models)\n",
    "                \n",
    "                preds = (avg_probs > FINAL_THRESHOLD).astype(int)\n",
    "                for oid, p in zip(batch_ids, preds): results[oid] = p\n",
    "                batch_ids, batch_seq, batch_static = [], [], []\n",
    "\n",
    "        if len(batch_ids) > 0:\n",
    "            X_seq = torch.tensor(batch_seq, dtype=torch.float32).to(DEVICE)\n",
    "            X_stat = torch.tensor(batch_static, dtype=torch.float32).to(DEVICE)\n",
    "            X_seq = torch.nan_to_num(X_seq, nan=0.0)\n",
    "            X_stat = torch.nan_to_num(X_stat, nan=0.0)\n",
    "            X_stat = (X_stat - mean_static) / std_static\n",
    "            X_stat = torch.nan_to_num(X_stat, nan=0.0)\n",
    "            \n",
    "            avg_probs = np.zeros(len(batch_ids))\n",
    "            for model in models:\n",
    "                avg_probs += torch.softmax(model(X_seq, X_stat), dim=1)[:, tde_index].cpu().numpy()\n",
    "            avg_probs /= len(models)\n",
    "            preds = (avg_probs > FINAL_THRESHOLD).astype(int)\n",
    "            for oid, p in zip(batch_ids, preds): \n",
    "                results[oid] = p\n",
    "        \n",
    "        del df_chunk, grouped\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({'object_id': df_test_log['object_id']})\n",
    "submission_df['prediction'] = submission_df['object_id'].map(results).fillna(0).astype(int)\n",
    "n_pos = submission_df['prediction'].sum()\n",
    "print(f\" Tìm thấy {n_pos} TDE trên tổng số {len(submission_df)} mẫu.\")\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8925232,
     "sourceId": 14010596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 170.124084,
   "end_time": "2026-01-31T09:11:43.470788",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-31T09:08:53.346704",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
